{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset, Image\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoImageProcessor, MaskFormerForInstanceSegmentation, TrainingArguments, Trainer, MaskFormerConfig, MaskFormerModel, Mask2FormerForUniversalSegmentation, Mask2FormerConfig, Mask2FormerModel\n",
    "from PIL import Image as PILImage\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "\n",
    "from typing import Dict, List, Mapping\n",
    "from transformers.trainer import EvalPrediction\n",
    "# from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "IMAGE_SIZE = (512, 512)  # Resize images to this size\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 5e-5\n",
    "VAL_SPLIT = 0.125\n",
    "\n",
    "id2label = {0: 'background', 1: 'water'}\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "NUM_CLASSES = len(id2label)\n",
    "\n",
    "# MODEL_CHECKPOINT = \"facebook/maskformer-resnet50-coco-stuff\"\n",
    "# MODEL_CHECKPOINT = \"facebook/maskformer-swin-large-ade\"\n",
    "MODEL_CHECKPOINT = \"facebook/mask2former-swin-large-cityscapes-semantic\"\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(MODEL_CHECKPOINT)\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir = \"./sar_images/images/train/*.png\"\n",
    "train_mask_dir = \"./sar_images/masks/train/*.png\"\n",
    "test_image_dir = \"./sar_images/images/test\"\n",
    "test_mask_dir = \"./sar_images/masks/test\"\n",
    "\n",
    "images = list(glob.glob(train_image_dir))\n",
    "# images = [str(path) for path in images]\n",
    "masks = [path.replace('/images', '/masks') for path in images]\n",
    "\n",
    "# print(images)\n",
    "# print(masks)\n",
    "\n",
    "# print(f'{len(images)} images detected.')\n",
    "\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "    images, masks, test_size=VAL_SPLIT, random_state=0, shuffle=True)\n",
    "\n",
    "print(f'Train images: {len(train_images)}\\nValidation images: {len(val_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(image_paths, mask_paths):\n",
    "    \"\"\"Creates a dataset storing file paths as individual strings, not lists\"\"\"\n",
    "    return Dataset.from_dict({\"pixel_values\": image_paths, \"label\": mask_paths})\n",
    "\n",
    "# Create dataset from file paths\n",
    "ds_train = create_dataset(train_images, train_masks)\n",
    "ds_valid = create_dataset(val_images, val_masks)\n",
    "\n",
    "def transform(example):\n",
    "    \"\"\"Loads images/masks from file paths and applies transformations using Albumentations.\"\"\"\n",
    "    \n",
    "    images = []\n",
    "    masks = []\n",
    "    \n",
    "    batch = {\n",
    "        \"pixel_values\": [],\n",
    "        \"mask_labels\": [],\n",
    "        \"class_labels\": [],\n",
    "    }\n",
    "    \n",
    "    for img_path, mask_path in zip(example[\"pixel_values\"], example[\"label\"]):\n",
    "        \n",
    "        image = PILImage.open(img_path).convert(\"RGB\")\n",
    "        mask = np.array(PILImage.open(mask_path).convert(\"L\"), dtype=np.uint8)  # Convert mask to grayscale\n",
    "        mask[mask == 255] = 1  # Convert 255 to 1\n",
    "        \n",
    "        images.append(image)\n",
    "        masks.append(mask)\n",
    "        \n",
    "        # print(mask.min(), mask.max())\n",
    "\n",
    "    # Process inputs using the Hugging Face processor\n",
    "    model_inputs = processor(images, segmentation_maps=masks, return_tensors='pt')\n",
    "    \n",
    "    batch[\"pixel_values\"].append(model_inputs.pixel_values[0])\n",
    "    batch[\"mask_labels\"].append(model_inputs.mask_labels[0])\n",
    "    batch[\"class_labels\"].append(model_inputs.class_labels[0])\n",
    "       \n",
    "    return batch\n",
    "\n",
    "# Apply transformation correctly\n",
    "ds_train.set_transform(transform)\n",
    "ds_valid.set_transform(transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelOutput:\n",
    "    class_queries_logits: torch.Tensor\n",
    "    masks_queries_logits: torch.Tensor\n",
    "\n",
    "def nested_cpu(tensors):\n",
    "    if isinstance(tensors, (list, tuple)):\n",
    "        return type(tensors)(nested_cpu(t) for t in tensors)\n",
    "    elif isinstance(tensors, Mapping):\n",
    "        return type(tensors)({k: nested_cpu(t) for k, t in tensors.items()})\n",
    "    elif isinstance(tensors, torch.Tensor):\n",
    "        return tensors.cpu().detach()\n",
    "    elif isinstance(tensors, np.ndarray):\n",
    "        return torch.from_numpy(tensors)\n",
    "    else:\n",
    "        return tensors\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Compute metrics for the instance segmentation task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_processor: AutoImageProcessor,\n",
    "        id2label: Mapping[int, str],\n",
    "        num_classes\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize evaluator with image processor, id2label mapping and threshold for filtering predictions.\n",
    "\n",
    "        Args:\n",
    "            image_processor (AutoImageProcessor): Image processor for\n",
    "                `post_process_instance_segmentation` method.\n",
    "            id2label (Mapping[int, str]): Mapping from class id to class name.\n",
    "        \"\"\"\n",
    "        self.image_processor = image_processor\n",
    "        self.id2label = id2label\n",
    "        # self.threshold = threshold\n",
    "        self.iou_metric = JaccardIndex(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "        self.acc_metric = Accuracy(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def reset_metric(self):\n",
    "        self.iou_metric.reset()\n",
    "        self.acc_metric.reset()\n",
    "    \n",
    "    def postprocess_target_batch(self, target_batch) -> List[Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Resize and process targets into dict format.\"\"\"\n",
    "        batch_masks, batch_labels = target_batch[0], target_batch[1]\n",
    "        # print(batch_masks.shape)\n",
    "\n",
    "        batch_masks_resized = F.interpolate(\n",
    "            torch.tensor(batch_masks),\n",
    "            size=IMAGE_SIZE,\n",
    "            mode=\"nearest\",\n",
    "        )\n",
    "        \n",
    "        post_processed_targets = [\n",
    "            {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n",
    "            for masks, labels in zip(batch_masks_resized, batch_labels)\n",
    "        ]\n",
    "        \n",
    "        return post_processed_targets\n",
    "    \n",
    "    def get_target_sizes(self, post_processed_targets) -> List[List[int]]:\n",
    "        \"\"\"Get sizes of target masks.\"\"\"\n",
    "        return [target[\"masks\"].shape[-2:] for target in post_processed_targets]\n",
    "\n",
    "        # return [list(IMAGE_SIZE) for _ in post_processed_targets]  # Ensure it's in [H, W] format\n",
    "\n",
    "    def postprocess_prediction_batch(self, prediction_batch, target_sizes) -> List[Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Collect predictions in a form of list of dictionaries with keys \"masks\", \"labels\", \"scores\".\"\"\"\n",
    "        \n",
    "        # print(\"Postprocess Prediction\")\n",
    "        \n",
    "        class_queries_logits = prediction_batch[0]\n",
    "        masks_queries_logits = prediction_batch[1]\n",
    "        \n",
    "        model_output = ModelOutput(class_queries_logits=class_queries_logits, masks_queries_logits=masks_queries_logits)\n",
    "        post_processed_output = self.image_processor.post_process_semantic_segmentation(\n",
    "            model_output, target_sizes=target_sizes\n",
    "        )\n",
    "\n",
    "        post_processed_predictions = []\n",
    "        for image_prediction in post_processed_output:  # No need for segments_info handling\n",
    "            post_processed_image_prediction = {\n",
    "                \"masks\": image_prediction.to(dtype=torch.long),  # Ensure it's a tensor of labels\n",
    "            }\n",
    "            post_processed_predictions.append(post_processed_image_prediction)\n",
    "\n",
    "        return post_processed_predictions\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, evaluation_results: EvalPrediction, compute_result: bool = True) -> Mapping[str, float]:\n",
    "        \"\"\"\n",
    "        Update metrics with current evaluation results and return metrics if `compute_result` is True.\n",
    "\n",
    "        Args:\n",
    "            evaluation_results (EvalPrediction): Predictions and targets from evaluation.\n",
    "            compute_result (bool): Whether to compute and return metrics.\n",
    "\n",
    "        Returns:\n",
    "            Mapping[str, float]: Metrics in a form of dictionary {<metric_name>: <metric_value>}\n",
    "        \"\"\"\n",
    "\n",
    "        prediction_batch = nested_cpu(evaluation_results.predictions)\n",
    "        target_batch = nested_cpu(evaluation_results.label_ids)\n",
    "\n",
    "        # print(\"Checkpoint 1\")\n",
    "        post_processed_targets = self.postprocess_target_batch(target_batch)\n",
    "        # print(\"Checkpoint 2\")\n",
    "        target_sizes = self.get_target_sizes(post_processed_targets)\n",
    "        # print(\"Checkpoint 3\")\n",
    "        post_processed_predictions = self.postprocess_prediction_batch(prediction_batch, target_sizes)\n",
    "        \n",
    "        # print(\"Checkpoint 4\")\n",
    "        \n",
    "        post_processed_predictions = [nested_cpu(item) for item in post_processed_predictions]\n",
    "        post_processed_targets = [nested_cpu(item) for item in post_processed_targets]\n",
    "\n",
    "        # print(\"Checkpoint 5\")\n",
    "        if not compute_result:\n",
    "            return\n",
    "          \n",
    "        for t in post_processed_targets:\n",
    "            t[\"masks\"] = torch.argmax(t[\"masks\"], dim=0, keepdim=False) \n",
    "        \n",
    "        pred_masks = torch.stack([p[\"masks\"] for p in post_processed_predictions])\n",
    "        target_masks = torch.stack([t[\"masks\"] for t in post_processed_targets]) \n",
    "        \n",
    "        iou_per_class = self.iou_metric(pred_masks, target_masks)\n",
    "        acc_per_class = self.acc_metric(pred_masks, target_masks)\n",
    "        \n",
    "        # print(\"Checkpoint 7\")\n",
    "\n",
    "        # Compute mean IoU & accuracy\n",
    "        mean_iou = iou_per_class.mean().item()\n",
    "        mean_acc = acc_per_class.mean().item()\n",
    "        overall_acc = (pred_masks == target_masks).float().mean().item()\n",
    "\n",
    "        # Get specific class IoU and accuracy\n",
    "        iou_background = iou_per_class[0].item()\n",
    "        iou_water = iou_per_class[1].item() if self.num_classes > 1 else -1\n",
    "        acc_background = acc_per_class[0].item()\n",
    "        acc_water = acc_per_class[1].item() if self.num_classes > 1 else -1\n",
    "        \n",
    "        # print(\"Checkpoint 8\")\n",
    "\n",
    "        metrics = {\n",
    "            \"mean_iou\": round(mean_iou, 4),\n",
    "            \"mean_accuracy\": round(mean_acc, 4),\n",
    "            \"overall_accuracy\": round(overall_acc, 4),\n",
    "            \"accuracy_background\": round(acc_background, 4),\n",
    "            \"accuracy_water\": round(acc_water, 4),\n",
    "            \"iou_background\": round(iou_background, 4),\n",
    "            \"iou_water\": round(iou_water, 4),\n",
    "        }\n",
    "\n",
    "        self.reset_metric()\n",
    "        # print(\"Checkpoint 9\")\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics = Evaluator(image_processor=processor, id2label=id2label, num_classes=NUM_CLASSES)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"maskformer_water_finetuned\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=False,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    push_to_hub=False,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # eval_do_concat_batches=False,\n",
    "    # dataloader_num_workers=8,\n",
    "    # dataloader_persistent_workers=True,\n",
    "    # dataloader_prefetch_factor=4,\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"overall_accuracy\",\n",
    "    eval_accumulation_steps=5,\n",
    "    report_to=None\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_valid,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('mask2former_water_new')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
