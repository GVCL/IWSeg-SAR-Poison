{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset, Image\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoImageProcessor, MaskFormerForInstanceSegmentation, TrainingArguments, Trainer, MaskFormerConfig, MaskFormerModel, MaskFormerImageProcessor\n",
    "from PIL import Image as PILImage\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from typing import Dict, List, Mapping\n",
    "from transformers.trainer import EvalPrediction\n",
    "# from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\transformers\\utils\\deprecation.py:172: UserWarning: The following named arguments are not valid for `MaskFormerImageProcessor.__init__` and were ignored: '_max_size'\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MaskFormerForInstanceSegmentation(\n",
       "  (model): MaskFormerModel(\n",
       "    (pixel_level_module): MaskFormerPixelLevelModule(\n",
       "      (encoder): ResNetBackbone(\n",
       "        (embedder): ResNetEmbeddings(\n",
       "          (embedder): ResNetConvLayer(\n",
       "            (convolution): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "            (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): ReLU()\n",
       "          )\n",
       "          (pooler): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (encoder): ResNetEncoder(\n",
       "          (stages): ModuleList(\n",
       "            (0): ResNetStage(\n",
       "              (layers): Sequential(\n",
       "                (0): ResNetBottleNeckLayer(\n",
       "                  (shortcut): ResNetShortCut(\n",
       "                    (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (layer): Sequential(\n",
       "                    (0): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ResNetStage(\n",
       "              (layers): Sequential(\n",
       "                (0): ResNetBottleNeckLayer(\n",
       "                  (shortcut): ResNetShortCut(\n",
       "                    (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                    (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (layer): Sequential(\n",
       "                    (0): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (3): ResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ResNetStage(\n",
       "              (layers): Sequential(\n",
       "                (0): ResNetBottleNeckLayer(\n",
       "                  (shortcut): ResNetShortCut(\n",
       "                    (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                    (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (layer): Sequential(\n",
       "                    (0): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (3): ResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (4): ResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (5): ResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): ResNetStage(\n",
       "              (layers): Sequential(\n",
       "                (0): ResNetBottleNeckLayer(\n",
       "                  (shortcut): ResNetShortCut(\n",
       "                    (convolution): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                    (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (layer): Sequential(\n",
       "                    (0): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): ResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (decoder): MaskFormerPixelDecoder(\n",
       "        (fpn): MaskFormerFPNModel(\n",
       "          (stem): MaskFormerFPNConvLayer(\n",
       "            (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (layers): Sequential(\n",
       "            (0): MaskFormerFPNLayer(\n",
       "              (proj): Sequential(\n",
       "                (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "              )\n",
       "              (block): MaskFormerFPNConvLayer(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): MaskFormerFPNLayer(\n",
       "              (proj): Sequential(\n",
       "                (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "              )\n",
       "              (block): MaskFormerFPNConvLayer(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): MaskFormerFPNLayer(\n",
       "              (proj): Sequential(\n",
       "                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "              )\n",
       "              (block): MaskFormerFPNConvLayer(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mask_projection): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (transformer_module): MaskFormerTransformerModule(\n",
       "      (position_embedder): MaskFormerSinePositionEmbedding()\n",
       "      (queries_embedder): Embedding(100, 256)\n",
       "      (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (decoder): DetrDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x DetrDecoderLayer(\n",
       "            (self_attn): DetrAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): DetrAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (class_predictor): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (mask_embedder): MaskformerMLPPredictionHead(\n",
       "    (0): PredictionBlock(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (1): PredictionBlock(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (2): PredictionBlock(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): Identity()\n",
       "    )\n",
       "  )\n",
       "  (matcher): Matcher MaskFormerHungarianMatcher\n",
       "      cost_class: 1.0\n",
       "      cost_mask: 20.0\n",
       "      cost_dice: 1.0\n",
       "  (criterion): MaskFormerLoss(\n",
       "    (matcher): Matcher MaskFormerHungarianMatcher\n",
       "        cost_class: 1.0\n",
       "        cost_mask: 20.0\n",
       "        cost_dice: 1.0\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "IMAGE_SIZE = (512, 512)  # Resize images to this size\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 5e-5\n",
    "VAL_SPLIT = 0.125\n",
    "\n",
    "id2label = {0: 'background', 1: 'water'}\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "NUM_CLASSES = len(id2label)\n",
    "\n",
    "MODEL_CHECKPOINT = \"facebook/maskformer-resnet50-coco-stuff\"\n",
    "\n",
    "config = MaskFormerConfig.from_pretrained(MODEL_CHECKPOINT)\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n",
    "config.num_labels = NUM_CLASSES\n",
    "\n",
    "# Use the config object to initialize a MaskFormer model with randomized weights\n",
    "model = MaskFormerForInstanceSegmentation(config)\n",
    "\n",
    "base_model = MaskFormerModel.from_pretrained(MODEL_CHECKPOINT)\n",
    "model.model = base_model\n",
    "\n",
    "# processor = AutoImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
    "processor = MaskFormerImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 883\n",
      "Validation images: 127\n"
     ]
    }
   ],
   "source": [
    "train_image_dir = \"./sar_images/images/train/*.png\"\n",
    "train_mask_dir = \"./sar_images/masks/train/*.png\"\n",
    "test_image_dir = \"./sar_images/images/test\"\n",
    "test_mask_dir = \"./sar_images/masks/test\"\n",
    "\n",
    "images = list(glob.glob(train_image_dir))\n",
    "# images = [str(path) for path in images]\n",
    "masks = [path.replace('/images', '/masks') for path in images]\n",
    "\n",
    "# print(images)\n",
    "# print(masks)\n",
    "\n",
    "# print(f'{len(images)} images detected.')\n",
    "\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "    images, masks, test_size=VAL_SPLIT, random_state=0, shuffle=True)\n",
    "\n",
    "print(f'Train images: {len(train_images)}\\nValidation images: {len(val_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(image_paths, mask_paths):\n",
    "    \"\"\"Creates a dataset storing file paths as individual strings, not lists\"\"\"\n",
    "    return Dataset.from_dict({\"pixel_values\": image_paths, \"label\": mask_paths})\n",
    "\n",
    "# Create dataset from file paths\n",
    "ds_train = create_dataset(train_images, train_masks)\n",
    "ds_valid = create_dataset(val_images, val_masks)\n",
    "\n",
    "alb_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE[0], IMAGE_SIZE[1]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "def transform(example):\n",
    "    \"\"\"Loads images/masks from file paths and applies transformations using Albumentations.\"\"\"\n",
    "    \n",
    "    images = []\n",
    "    masks = []\n",
    "    \n",
    "    batch = {\n",
    "        \"pixel_values\": [],\n",
    "        \"mask_labels\": [],\n",
    "        \"class_labels\": [],\n",
    "    }\n",
    "    \n",
    "    for img_path, mask_path in zip(example[\"pixel_values\"], example[\"label\"]):\n",
    "        \n",
    "        # print(img_path, mask_path)\n",
    "        \n",
    "        # Open images and masks\n",
    "        image = np.array(PILImage.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(PILImage.open(mask_path).convert(\"L\"), dtype=np.uint8)  # Convert mask to grayscale\n",
    "\n",
    "        mask[mask == 255] = 1  # Convert 255 to 1\n",
    "        # Apply Albumentations transform\n",
    "        transformed = alb_transform(image=image, mask=mask)\n",
    "        \n",
    "        # Extract transformed image and mask\n",
    "        image = transformed[\"image\"]\n",
    "        mask = transformed[\"mask\"].long()  # Ensure mask is long tensor\n",
    "        \n",
    "        images.append(image)\n",
    "        masks.append(mask)\n",
    "        \n",
    "        # print(mask.min(), mask.max())\n",
    "\n",
    "    # Process inputs using the Hugging Face processor\n",
    "    model_inputs = processor(images, segmentation_maps=masks, return_tensors='pt')\n",
    "    \n",
    "    batch[\"pixel_values\"].append(model_inputs.pixel_values[0])\n",
    "    batch[\"mask_labels\"].append(model_inputs.mask_labels[0])\n",
    "    batch[\"class_labels\"].append(model_inputs.class_labels[0])\n",
    "       \n",
    "    return batch\n",
    "\n",
    "\n",
    "# def collate_fn(examples):\n",
    "#     batch = {}\n",
    "#     batch[\"pixel_values\"] = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "#     batch[\"class_labels\"] = [example[\"class_labels\"] for example in examples]\n",
    "#     batch[\"mask_labels\"] = [example[\"mask_labels\"] for example in examples]\n",
    "#     if \"pixel_mask\" in examples[0]:\n",
    "#         batch[\"pixel_mask\"] = torch.stack([example[\"pixel_mask\"] for example in examples])\n",
    "#     return batch\n",
    "\n",
    "# Apply transformation correctly\n",
    "ds_train.set_transform(transform)\n",
    "ds_valid.set_transform(transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelOutput:\n",
    "    class_queries_logits: torch.Tensor\n",
    "    masks_queries_logits: torch.Tensor\n",
    "\n",
    "def nested_cpu(tensors):\n",
    "    if isinstance(tensors, (list, tuple)):\n",
    "        return type(tensors)(nested_cpu(t) for t in tensors)\n",
    "    elif isinstance(tensors, Mapping):\n",
    "        return type(tensors)({k: nested_cpu(t) for k, t in tensors.items()})\n",
    "    elif isinstance(tensors, torch.Tensor):\n",
    "        return tensors.cpu().detach()\n",
    "    elif isinstance(tensors, np.ndarray):\n",
    "        return torch.from_numpy(tensors)\n",
    "    else:\n",
    "        return tensors\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Compute metrics for the instance segmentation task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_processor: AutoImageProcessor,\n",
    "        id2label: Mapping[int, str],\n",
    "        num_classes\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize evaluator with image processor, id2label mapping and threshold for filtering predictions.\n",
    "\n",
    "        Args:\n",
    "            image_processor (AutoImageProcessor): Image processor for\n",
    "                `post_process_instance_segmentation` method.\n",
    "            id2label (Mapping[int, str]): Mapping from class id to class name.\n",
    "            threshold (float): Threshold to filter predicted boxes by confidence. Defaults to 0.0.\n",
    "        \"\"\"\n",
    "        self.image_processor = image_processor\n",
    "        self.id2label = id2label\n",
    "        # self.threshold = threshold\n",
    "        self.iou_metric = JaccardIndex(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "        self.acc_metric = Accuracy(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def reset_metric(self):\n",
    "        self.iou_metric.reset()\n",
    "        self.acc_metric.reset()\n",
    "\n",
    "    def postprocess_target_batch(self, target_batch) -> List[Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Process targets into dict format.\"\"\"\n",
    "        # print(type(target_batch))\n",
    "        # print(len(target_batch))\n",
    "        batch_masks, batch_labels = target_batch[0], target_batch[1]\n",
    "        \n",
    "        # print(type(batch_masks))\n",
    "        # print(type(batch_labels))\n",
    "        # print(len(batch_masks))\n",
    "        # print(len(batch_labels))\n",
    "        \n",
    "        # print(type(batch_masks[0]))\n",
    "        # print(batch_masks[0].shape)\n",
    "        # print(type(batch_labels[0]))\n",
    "        # print(batch_labels[0].shape)\n",
    "        # print(len(batch_masks))\n",
    "        # print(len(batch_labels))\n",
    "        \n",
    "        # print(batch_masks.shape)\n",
    "        # print(batch_labels.shape)\n",
    "        # print(torch.tensor(masks).shape, torch.tensor(labels).shape)\n",
    "        \n",
    "        post_processed_targets = [\n",
    "            {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n",
    "            for masks, labels in zip(batch_masks, batch_labels)\n",
    "        ]\n",
    "        \n",
    "        return post_processed_targets\n",
    "    \n",
    "    def get_target_sizes(self, post_processed_targets) -> List[List[int]]:\n",
    "        \"\"\"Get sizes of target masks.\"\"\"\n",
    "        return [target[\"masks\"].shape[-2:] for target in post_processed_targets]\n",
    "\n",
    "    def postprocess_prediction_batch(self, prediction_batch, target_sizes) -> List[Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Collect predictions in a form of list of dictionaries with keys \"masks\", \"labels\", \"scores\".\"\"\"\n",
    "        \n",
    "        # print(\"Postprocess Prediction\")\n",
    "        \n",
    "        class_queries_logits = prediction_batch[0]\n",
    "        masks_queries_logits = prediction_batch[1]\n",
    "        \n",
    "        # print(\"Here 1\")\n",
    "\n",
    "        model_output = ModelOutput(class_queries_logits=class_queries_logits, masks_queries_logits=masks_queries_logits)\n",
    "        post_processed_output = self.image_processor.post_process_semantic_segmentation(\n",
    "            model_output, target_sizes=target_sizes\n",
    "        )\n",
    "\n",
    "        # print(\"Here 2\")\n",
    "\n",
    "        post_processed_predictions = []\n",
    "        for image_prediction in post_processed_output:  # No need for segments_info handling\n",
    "            post_processed_image_prediction = {\n",
    "                \"masks\": image_prediction.to(dtype=torch.long),  # Ensure it's a tensor of labels\n",
    "            }\n",
    "            post_processed_predictions.append(post_processed_image_prediction)\n",
    "\n",
    "        return post_processed_predictions\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, evaluation_results: EvalPrediction, compute_result: bool = True) -> Mapping[str, float]:\n",
    "        \"\"\"\n",
    "        Update metrics with current evaluation results and return metrics if `compute_result` is True.\n",
    "\n",
    "        Args:\n",
    "            evaluation_results (EvalPrediction): Predictions and targets from evaluation.\n",
    "            compute_result (bool): Whether to compute and return metrics.\n",
    "\n",
    "        Returns:\n",
    "            Mapping[str, float]: Metrics in a form of dictionary {<metric_name>: <metric_value>}\n",
    "        \"\"\"\n",
    "\n",
    "        prediction_batch = nested_cpu(evaluation_results.predictions)\n",
    "        target_batch = nested_cpu(evaluation_results.label_ids)\n",
    "\n",
    "        # print(\"Checkpoint 1\")\n",
    "        post_processed_targets = self.postprocess_target_batch(target_batch)\n",
    "        # print(len(post_processed_targets))\n",
    "        # print(\"Checkpoint 2\")\n",
    "        target_sizes = self.get_target_sizes(post_processed_targets)\n",
    "        # print(len(target_sizes))\n",
    "        # print(\"Checkpoint 3\")\n",
    "        post_processed_predictions = self.postprocess_prediction_batch(prediction_batch, target_sizes)\n",
    "        \n",
    "        # print(\"Checkpoint 4\")\n",
    "        \n",
    "        post_processed_predictions = [nested_cpu(item) for item in post_processed_predictions]\n",
    "        post_processed_targets = [nested_cpu(item) for item in post_processed_targets]\n",
    "\n",
    "        # print(\"Checkpoint 5\")\n",
    "        if not compute_result:\n",
    "            return\n",
    "            \n",
    "        pred_masks = torch.stack([p[\"masks\"] for p in post_processed_predictions])\n",
    "        \n",
    "        for t in post_processed_targets:\n",
    "            t[\"masks\"] = torch.argmax(t[\"masks\"], dim=0, keepdim=False)\n",
    "        \n",
    "        target_masks = torch.stack([t[\"masks\"] for t in post_processed_targets]) \n",
    "        \n",
    "        # print(\"Checkpoint 6\")\n",
    "        # Compute metrics\n",
    "        \n",
    "        print(pred_masks.min(), pred_masks.max())\n",
    "        print(target_masks.min(), target_masks.max())\n",
    "        \n",
    "        # self.iou_metric.update(pred_masks, target_masks)\n",
    "        # self.acc_metric.update(pred_masks, target_masks)\n",
    "        # iou_metrics = self.iou_metric.compute()\n",
    "        # acc_metrics = self.acc_metric.compute()\n",
    "        \n",
    "        iou_per_class = self.iou_metric(pred_masks, target_masks)\n",
    "        acc_per_class = self.acc_metric(pred_masks, target_masks)\n",
    "        \n",
    "        # print(\"Checkpoint 7\")\n",
    "\n",
    "        # Compute mean IoU & accuracy\n",
    "        mean_iou = iou_per_class.mean().item()\n",
    "        mean_acc = acc_per_class.mean().item()\n",
    "        overall_acc = (pred_masks == target_masks).float().mean().item()\n",
    "\n",
    "        # Get specific class IoU and accuracy\n",
    "        iou_background = iou_per_class[0].item()\n",
    "        iou_water = iou_per_class[1].item() if self.num_classes > 1 else -1\n",
    "        acc_background = acc_per_class[0].item()\n",
    "        acc_water = acc_per_class[1].item() if self.num_classes > 1 else -1\n",
    "        \n",
    "        # print(\"Checkpoint 8\")\n",
    "\n",
    "        metrics = {\n",
    "            \"mean_iou\": round(mean_iou, 4),\n",
    "            \"mean_accuracy\": round(mean_acc, 4),\n",
    "            \"overall_accuracy\": round(overall_acc, 4),\n",
    "            \"accuracy_background\": round(acc_background, 4),\n",
    "            \"accuracy_water\": round(acc_water, 4),\n",
    "            \"iou_background\": round(iou_background, 4),\n",
    "            \"iou_water\": round(iou_water, 4),\n",
    "        }\n",
    "\n",
    "        self.reset_metric()\n",
    "        # print(\"Checkpoint 9\")\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5500' max='5500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5500/5500 1:23:55, Epoch 49/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mean Iou</th>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>Accuracy Background</th>\n",
       "      <th>Accuracy Water</th>\n",
       "      <th>Iou Background</th>\n",
       "      <th>Iou Water</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.428400</td>\n",
       "      <td>12.590865</td>\n",
       "      <td>0.126600</td>\n",
       "      <td>0.500200</td>\n",
       "      <td>0.252900</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.999800</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.875300</td>\n",
       "      <td>1.574219</td>\n",
       "      <td>0.461600</td>\n",
       "      <td>0.594800</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>0.833700</td>\n",
       "      <td>0.355800</td>\n",
       "      <td>0.684700</td>\n",
       "      <td>0.238400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.291900</td>\n",
       "      <td>2.267871</td>\n",
       "      <td>0.626400</td>\n",
       "      <td>0.734100</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.932800</td>\n",
       "      <td>0.535400</td>\n",
       "      <td>0.806300</td>\n",
       "      <td>0.446500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.081900</td>\n",
       "      <td>1.327680</td>\n",
       "      <td>0.616700</td>\n",
       "      <td>0.837000</td>\n",
       "      <td>0.780400</td>\n",
       "      <td>0.722700</td>\n",
       "      <td>0.951300</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>0.522500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.061100</td>\n",
       "      <td>2.302904</td>\n",
       "      <td>0.783500</td>\n",
       "      <td>0.850500</td>\n",
       "      <td>0.911700</td>\n",
       "      <td>0.974100</td>\n",
       "      <td>0.726900</td>\n",
       "      <td>0.891800</td>\n",
       "      <td>0.675200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.183900</td>\n",
       "      <td>3.193964</td>\n",
       "      <td>0.664400</td>\n",
       "      <td>0.830900</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.821100</td>\n",
       "      <td>0.840700</td>\n",
       "      <td>0.779200</td>\n",
       "      <td>0.549600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.836300</td>\n",
       "      <td>1.418788</td>\n",
       "      <td>0.747600</td>\n",
       "      <td>0.813900</td>\n",
       "      <td>0.898800</td>\n",
       "      <td>0.985500</td>\n",
       "      <td>0.642400</td>\n",
       "      <td>0.879200</td>\n",
       "      <td>0.615900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.939100</td>\n",
       "      <td>3.418244</td>\n",
       "      <td>0.606800</td>\n",
       "      <td>0.694300</td>\n",
       "      <td>0.843100</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>0.393600</td>\n",
       "      <td>0.825800</td>\n",
       "      <td>0.387800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.875200</td>\n",
       "      <td>2.796201</td>\n",
       "      <td>0.789800</td>\n",
       "      <td>0.846800</td>\n",
       "      <td>0.916600</td>\n",
       "      <td>0.987700</td>\n",
       "      <td>0.705900</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>0.681200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.036300</td>\n",
       "      <td>2.010036</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.705600</td>\n",
       "      <td>0.816700</td>\n",
       "      <td>0.929900</td>\n",
       "      <td>0.481400</td>\n",
       "      <td>0.791300</td>\n",
       "      <td>0.398700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.631700</td>\n",
       "      <td>4.915873</td>\n",
       "      <td>0.795500</td>\n",
       "      <td>0.871200</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.958300</td>\n",
       "      <td>0.784100</td>\n",
       "      <td>0.893200</td>\n",
       "      <td>0.697900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.819100</td>\n",
       "      <td>1.445932</td>\n",
       "      <td>0.793100</td>\n",
       "      <td>0.853200</td>\n",
       "      <td>0.917000</td>\n",
       "      <td>0.982000</td>\n",
       "      <td>0.724500</td>\n",
       "      <td>0.898400</td>\n",
       "      <td>0.687800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.650100</td>\n",
       "      <td>1.653609</td>\n",
       "      <td>0.773100</td>\n",
       "      <td>0.831900</td>\n",
       "      <td>0.910200</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.673900</td>\n",
       "      <td>0.891700</td>\n",
       "      <td>0.654500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.679200</td>\n",
       "      <td>1.541544</td>\n",
       "      <td>0.805900</td>\n",
       "      <td>0.857100</td>\n",
       "      <td>0.923700</td>\n",
       "      <td>0.991700</td>\n",
       "      <td>0.722400</td>\n",
       "      <td>0.906700</td>\n",
       "      <td>0.705100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>2.005992</td>\n",
       "      <td>0.759400</td>\n",
       "      <td>0.823400</td>\n",
       "      <td>0.903800</td>\n",
       "      <td>0.985800</td>\n",
       "      <td>0.661000</td>\n",
       "      <td>0.884500</td>\n",
       "      <td>0.634300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.623500</td>\n",
       "      <td>1.982230</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.765100</td>\n",
       "      <td>0.873500</td>\n",
       "      <td>0.984100</td>\n",
       "      <td>0.546200</td>\n",
       "      <td>0.853300</td>\n",
       "      <td>0.521600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.619700</td>\n",
       "      <td>2.313910</td>\n",
       "      <td>0.526300</td>\n",
       "      <td>0.628100</td>\n",
       "      <td>0.808200</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.794400</td>\n",
       "      <td>0.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.586800</td>\n",
       "      <td>2.769973</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>0.864500</td>\n",
       "      <td>0.915500</td>\n",
       "      <td>0.967400</td>\n",
       "      <td>0.761600</td>\n",
       "      <td>0.895300</td>\n",
       "      <td>0.694600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.594600</td>\n",
       "      <td>2.180603</td>\n",
       "      <td>0.795800</td>\n",
       "      <td>0.856900</td>\n",
       "      <td>0.917700</td>\n",
       "      <td>0.979800</td>\n",
       "      <td>0.734000</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>0.692600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.536500</td>\n",
       "      <td>1.446127</td>\n",
       "      <td>0.783600</td>\n",
       "      <td>0.845500</td>\n",
       "      <td>0.913100</td>\n",
       "      <td>0.982000</td>\n",
       "      <td>0.709100</td>\n",
       "      <td>0.894100</td>\n",
       "      <td>0.673200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.556500</td>\n",
       "      <td>1.229384</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.895500</td>\n",
       "      <td>0.938700</td>\n",
       "      <td>0.982800</td>\n",
       "      <td>0.808100</td>\n",
       "      <td>0.923000</td>\n",
       "      <td>0.769000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.596100</td>\n",
       "      <td>2.318531</td>\n",
       "      <td>0.784400</td>\n",
       "      <td>0.848900</td>\n",
       "      <td>0.912700</td>\n",
       "      <td>0.977700</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.893200</td>\n",
       "      <td>0.675500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.555700</td>\n",
       "      <td>2.007764</td>\n",
       "      <td>0.830200</td>\n",
       "      <td>0.889100</td>\n",
       "      <td>0.931200</td>\n",
       "      <td>0.974100</td>\n",
       "      <td>0.804100</td>\n",
       "      <td>0.913600</td>\n",
       "      <td>0.746800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.590300</td>\n",
       "      <td>2.219839</td>\n",
       "      <td>0.470100</td>\n",
       "      <td>0.726600</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.573900</td>\n",
       "      <td>0.879300</td>\n",
       "      <td>0.551400</td>\n",
       "      <td>0.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.611200</td>\n",
       "      <td>1.390196</td>\n",
       "      <td>0.630100</td>\n",
       "      <td>0.814600</td>\n",
       "      <td>0.799300</td>\n",
       "      <td>0.783700</td>\n",
       "      <td>0.845500</td>\n",
       "      <td>0.744800</td>\n",
       "      <td>0.515400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.522700</td>\n",
       "      <td>1.737043</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.742300</td>\n",
       "      <td>0.710800</td>\n",
       "      <td>0.678800</td>\n",
       "      <td>0.805700</td>\n",
       "      <td>0.637000</td>\n",
       "      <td>0.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.531700</td>\n",
       "      <td>2.125538</td>\n",
       "      <td>0.617700</td>\n",
       "      <td>0.767100</td>\n",
       "      <td>0.804700</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.763400</td>\n",
       "      <td>0.471900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.535300</td>\n",
       "      <td>1.925964</td>\n",
       "      <td>0.720700</td>\n",
       "      <td>0.803200</td>\n",
       "      <td>0.883100</td>\n",
       "      <td>0.964600</td>\n",
       "      <td>0.641700</td>\n",
       "      <td>0.860500</td>\n",
       "      <td>0.580900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.483900</td>\n",
       "      <td>1.421193</td>\n",
       "      <td>0.852200</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.939000</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.869600</td>\n",
       "      <td>0.921900</td>\n",
       "      <td>0.782600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>1.816469</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>0.788500</td>\n",
       "      <td>0.843700</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.677100</td>\n",
       "      <td>0.811500</td>\n",
       "      <td>0.522400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>2.835062</td>\n",
       "      <td>0.633200</td>\n",
       "      <td>0.809200</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.798800</td>\n",
       "      <td>0.819600</td>\n",
       "      <td>0.752900</td>\n",
       "      <td>0.513600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.456700</td>\n",
       "      <td>3.753154</td>\n",
       "      <td>0.436500</td>\n",
       "      <td>0.667700</td>\n",
       "      <td>0.624600</td>\n",
       "      <td>0.580500</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.536100</td>\n",
       "      <td>0.336800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.469800</td>\n",
       "      <td>1.838024</td>\n",
       "      <td>0.591800</td>\n",
       "      <td>0.771000</td>\n",
       "      <td>0.775500</td>\n",
       "      <td>0.780100</td>\n",
       "      <td>0.761800</td>\n",
       "      <td>0.722100</td>\n",
       "      <td>0.461500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.407000</td>\n",
       "      <td>1.508552</td>\n",
       "      <td>0.735400</td>\n",
       "      <td>0.856600</td>\n",
       "      <td>0.876900</td>\n",
       "      <td>0.897700</td>\n",
       "      <td>0.815500</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.625900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.454300</td>\n",
       "      <td>1.535444</td>\n",
       "      <td>0.835300</td>\n",
       "      <td>0.891700</td>\n",
       "      <td>0.933500</td>\n",
       "      <td>0.976100</td>\n",
       "      <td>0.807400</td>\n",
       "      <td>0.916500</td>\n",
       "      <td>0.754100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>1.386142</td>\n",
       "      <td>0.811200</td>\n",
       "      <td>0.895200</td>\n",
       "      <td>0.919000</td>\n",
       "      <td>0.943300</td>\n",
       "      <td>0.847200</td>\n",
       "      <td>0.897000</td>\n",
       "      <td>0.725400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.489900</td>\n",
       "      <td>1.530031</td>\n",
       "      <td>0.736600</td>\n",
       "      <td>0.849400</td>\n",
       "      <td>0.879700</td>\n",
       "      <td>0.910700</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.849900</td>\n",
       "      <td>0.623300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.418900</td>\n",
       "      <td>1.399961</td>\n",
       "      <td>0.735500</td>\n",
       "      <td>0.867000</td>\n",
       "      <td>0.874300</td>\n",
       "      <td>0.881600</td>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.839800</td>\n",
       "      <td>0.631300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.495800</td>\n",
       "      <td>1.662760</td>\n",
       "      <td>0.720600</td>\n",
       "      <td>0.820900</td>\n",
       "      <td>0.876500</td>\n",
       "      <td>0.933300</td>\n",
       "      <td>0.708600</td>\n",
       "      <td>0.849600</td>\n",
       "      <td>0.591700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.483300</td>\n",
       "      <td>1.994371</td>\n",
       "      <td>0.631900</td>\n",
       "      <td>0.724500</td>\n",
       "      <td>0.845400</td>\n",
       "      <td>0.968800</td>\n",
       "      <td>0.480200</td>\n",
       "      <td>0.824100</td>\n",
       "      <td>0.439600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.399900</td>\n",
       "      <td>1.607554</td>\n",
       "      <td>0.760100</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.893200</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.799100</td>\n",
       "      <td>0.866200</td>\n",
       "      <td>0.653900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>2.140985</td>\n",
       "      <td>0.663000</td>\n",
       "      <td>0.741500</td>\n",
       "      <td>0.865400</td>\n",
       "      <td>0.991700</td>\n",
       "      <td>0.491300</td>\n",
       "      <td>0.846300</td>\n",
       "      <td>0.479600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.424400</td>\n",
       "      <td>1.890773</td>\n",
       "      <td>0.825500</td>\n",
       "      <td>0.883900</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.975900</td>\n",
       "      <td>0.791800</td>\n",
       "      <td>0.911800</td>\n",
       "      <td>0.739200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.370700</td>\n",
       "      <td>1.881915</td>\n",
       "      <td>0.814200</td>\n",
       "      <td>0.874600</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.773100</td>\n",
       "      <td>0.906500</td>\n",
       "      <td>0.721800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.431600</td>\n",
       "      <td>2.243932</td>\n",
       "      <td>0.779500</td>\n",
       "      <td>0.851900</td>\n",
       "      <td>0.908800</td>\n",
       "      <td>0.966900</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.671100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.419100</td>\n",
       "      <td>1.573621</td>\n",
       "      <td>0.833600</td>\n",
       "      <td>0.889700</td>\n",
       "      <td>0.932900</td>\n",
       "      <td>0.977000</td>\n",
       "      <td>0.802500</td>\n",
       "      <td>0.915900</td>\n",
       "      <td>0.751300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.368600</td>\n",
       "      <td>1.683261</td>\n",
       "      <td>0.815900</td>\n",
       "      <td>0.889700</td>\n",
       "      <td>0.922800</td>\n",
       "      <td>0.956600</td>\n",
       "      <td>0.822800</td>\n",
       "      <td>0.902600</td>\n",
       "      <td>0.729200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.392800</td>\n",
       "      <td>1.869999</td>\n",
       "      <td>0.828700</td>\n",
       "      <td>0.895700</td>\n",
       "      <td>0.929200</td>\n",
       "      <td>0.963300</td>\n",
       "      <td>0.828100</td>\n",
       "      <td>0.910400</td>\n",
       "      <td>0.747000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.358300</td>\n",
       "      <td>1.632726</td>\n",
       "      <td>0.843500</td>\n",
       "      <td>0.899300</td>\n",
       "      <td>0.936800</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.823600</td>\n",
       "      <td>0.920200</td>\n",
       "      <td>0.766800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23660\\3736340452.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {\"masks\": torch.tensor(masks, dtype=torch.long), \"labels\": torch.tensor(labels)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5500, training_loss=0.7003448947559704, metrics={'train_runtime': 5036.8528, 'train_samples_per_second': 8.765, 'train_steps_per_second': 1.092, 'total_flos': 5.20318690418304e+18, 'train_loss': 0.7003448947559704, 'epoch': 49.55203619909502})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics = Evaluator(image_processor=processor, id2label=id2label, num_classes=NUM_CLASSES)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"maskformer_water_finetuned\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=False,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    push_to_hub=False,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # eval_do_concat_batches=False,\n",
    "    # dataloader_num_workers=8,\n",
    "    # dataloader_persistent_workers=True,\n",
    "    # dataloader_prefetch_factor=4,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_accumulation_steps=5,\n",
    "    report_to=None\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=ds_train,\n",
    "#     eval_dataset=ds_valid,\n",
    "#     data_collator=collate_fn,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_valid,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('maskformer_water')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
