{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset, Image\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoImageProcessor, MaskFormerForInstanceSegmentation, TrainingArguments, Trainer, MaskFormerConfig, MaskFormerModel, MaskFormerImageProcessor, Mask2FormerForUniversalSegmentation, Mask2FormerConfig, Mask2FormerModel\n",
    "from PIL import Image as PILImage\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "\n",
    "from typing import Dict, List, Mapping\n",
    "from transformers.trainer import EvalPrediction\n",
    "# from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "IMAGE_SIZE = (512, 512)  # Resize images to this size\n",
    "\n",
    "id2label = {0: 'background', 1: 'water'}\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "NUM_CLASSES = len(id2label)\n",
    "\n",
    "MODEL_CHECKPOINT = \"mask2former_water_new\"\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(MODEL_CHECKPOINT)\n",
    "# model = MaskFormerForInstanceSegmentation.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# MODEL_CHECKPOINT = \"facebook/maskformer-swin-large-ade\"\n",
    "MODEL_CHECKPOINT = \"facebook/mask2former-swin-large-cityscapes-semantic\"\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_dir = \"./sar_images/images/test/*.png\"\n",
    "# test_mask_dir = \"./sar_images/masks/test/*.png\"\n",
    "\n",
    "test_images_paths = list(glob.glob(test_image_dir))\n",
    "# images = [str(path) for path in images]\n",
    "test_masks_paths = [path.replace('/images', '/masks') for path in test_images_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alb_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE[0], IMAGE_SIZE[1]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "def create_dataset(image_paths, mask_paths):\n",
    "    \n",
    "    images = []\n",
    "    masks = []\n",
    "    \n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        \n",
    "        image = PILImage.open(img_path).convert(\"RGB\")\n",
    "        mask = np.array(PILImage.open(mask_path).convert(\"L\"), dtype=np.uint8)  # Convert mask to grayscale\n",
    "        mask[mask == 255] = 1  \n",
    "        \n",
    "        images.append(image)\n",
    "        masks.append(mask)\n",
    "                \n",
    "    return images, masks\n",
    "\n",
    "test_images, test_masks = create_dataset(test_images_paths, test_masks_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, mask, pred, idx, name):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle(f'Sample {idx}', fontsize=16)\n",
    "\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(mask, cmap='gray')\n",
    "    axes[1].set_title('Ground Truth')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(pred, cmap='gray')\n",
    "    axes[2].set_title('Prediction')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.savefig(f\"./GEE_Output/Maskformer/Visualizations/{name}\")\n",
    "    plt.show()\n",
    "    \n",
    "output_dir = \"./GEE_Output/Maskformer/Outputs\"\n",
    "for i in range(len(test_images)):\n",
    "    \n",
    "    image = test_images[i]\n",
    "    \n",
    "    # image = test_images[i].cpu().numpy()\n",
    "    # image = PILImage.fromarray(image.transpose(1, 2, 0), mode='RGB')    \n",
    "    mask = test_masks[i]\n",
    "    name = test_images_paths[i]\n",
    "    \n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    class_queries_logits = outputs.class_queries_logits\n",
    "    masks_queries_logits = outputs.masks_queries_logits   \n",
    "    \n",
    "    predicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n",
    "    name = os.path.basename(name)\n",
    "    pred_mask = predicted_semantic_map.cpu().numpy()\n",
    "    \n",
    "    pred_norm = pred_mask * 255\n",
    "    pred_norm = pred_norm.astype(np.uint8)\n",
    "    pred_path = os.path.join(output_dir, f\"pred_{name}\")\n",
    "    cv2.imwrite(pred_path, pred_norm)\n",
    "    visualize(np.array(image), mask, pred_mask, i, name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
