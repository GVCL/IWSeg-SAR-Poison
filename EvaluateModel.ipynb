{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "import cv2\n",
    "import re\n",
    "# import os\n",
    "import glob\n",
    "# import re\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n",
    "import tifffile as tiff\n",
    "# import os\n",
    "# import numpy as np\n",
    "import glob\n",
    "# from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n",
    "# import rasterio\n",
    "\n",
    "import json\n",
    "\n",
    "# with open('config.json') as json_file:\n",
    "#     config = json.load(json_file)\n",
    "#     corruption = config['corruption_level']\n",
    "\n",
    "\n",
    "def load_image(filepath):\n",
    "    # Load the image using PIL\n",
    "    image = np.array(Image.open(filepath).convert('L'))\n",
    "    \n",
    "    # Apply Otsu's thresholding\n",
    "    t, binary_image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Convert to binary (0 and 1)\n",
    "    binary_image = (binary_image > 0).astype(np.uint8)\n",
    "    return binary_image\n",
    "\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    return (2. * intersection) / (np.sum(y_true) + np.sum(y_pred))\n",
    "\n",
    "def pixel_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "# Paths to the folders containing the predictions and ground truths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred, smooth=1):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n",
    "\n",
    "def pixel_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "\n",
    "# Directories for ground truth and prediction masks (Unet)\n",
    "# gt_folder = './GEE_Masks/GEE_resized/'\n",
    "# Directories for ground truth masks (DeepLabV3, Segformer, Maskformer)\n",
    "gt_folder = './sar_images/masks/test'\n",
    "\n",
    "\n",
    "# For UNet\n",
    "# pred_folder = f'./GEE_Output/Adversarial/New_20_Epoch_{corruption}_with_diff_kernels'\n",
    "# pred_files = glob.glob(os.path.join(pred_folder, 'dense_*.tif'))\n",
    "\n",
    "# For DeepLabV3\n",
    "pred_folder = f'./GEE_Output/Segformer/Outputs'\n",
    "\n",
    "# For Segformer\n",
    "# pred_folder = f'./GEE_Output/Segformer/Outputs'\n",
    "\n",
    "# For Maskformer\n",
    "# pred_folder = f'./GEE_Output/Maskformer/Outputs'\n",
    "\n",
    "print(pred_folder)\n",
    "\n",
    "# Initialize metric lists\n",
    "dice_scores = []\n",
    "iou_scores = []\n",
    "pixel_accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "specificities = []\n",
    "\n",
    "# Get all prediction files\n",
    "\n",
    "# pred_files = glob.glob(os.path.join(pred_folder, 'mobilenetv2_*.tif'))\n",
    "# For DeepLabV3\n",
    "pred_files = glob.glob(os.path.join(pred_folder, '*.png'))\n",
    "\n",
    "for pred_file in pred_files:\n",
    "    # Extract the number 'i' from the prediction filename\n",
    "    # For UNet\n",
    "    # match = re.search(r'dense_(\\d+).tif', os.path.basename(pred_file))\n",
    "    # For DeeplabV3, Segformer, Maskformer\n",
    "    match = re.search(r'pred_(\\d+)\\.png', os.path.basename(pred_file))\n",
    "    \n",
    "    if match:\n",
    "        i = match.group(1)\n",
    "        \n",
    "        # For UNet\n",
    "        # gt_filename = f'NDWI_Mask_{i}_resized.tif'\n",
    "        \n",
    "        # For Deeplabv3, Segformer and Maskformer\n",
    "        gt_filename = f'{i}.png'\n",
    "        \n",
    "        gt_path = os.path.join(gt_folder, gt_filename)\n",
    "        \n",
    "        # Check if the ground truth file exists\n",
    "        if os.path.exists(gt_path):\n",
    "            # Load the prediction and ground truth images\n",
    "            pred_image = load_image(pred_file)\n",
    "            gt_image = load_image(gt_path)\n",
    "            print(np.max(pred_image))\n",
    "            print(np.min(pred_image))\n",
    "            # Flatten the images\n",
    "            pred_flat = pred_image.flatten()\n",
    "            gt_flat = gt_image.flatten()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            dice = dice_coefficient(gt_flat, pred_flat)\n",
    "            iou = jaccard_score(gt_flat, pred_flat, average='macro')\n",
    "            accuracy = pixel_accuracy(gt_flat, pred_flat)\n",
    "            precision = precision_score(gt_flat, pred_flat, average='macro', zero_division=0)\n",
    "            recall = recall_score(gt_flat, pred_flat, average='macro')\n",
    "            f1 = f1_score(gt_flat, pred_flat, average='macro')\n",
    "            spec = specificity(gt_flat, pred_flat)\n",
    "            \n",
    "            # Store metrics\n",
    "            dice_scores.append(dice)\n",
    "            iou_scores.append(iou)\n",
    "            pixel_accuracies.append(accuracy)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            specificities.append(spec)\n",
    "            \n",
    "            # Print metrics for the current image\n",
    "            print(f\"Metrics for {gt_filename} and {pred_file}:\")\n",
    "            print(f\"  Dice Coefficient: {dice:.4f}\")\n",
    "            print(f\"  IoU: {iou:.4f}\")\n",
    "            print(f\"  Pixel Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall: {recall:.4f}\")\n",
    "            print(f\"  F1 Score: {f1:.4f}\")\n",
    "            print(f\"  Specificity: {spec:.4f}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"Ground truth file {gt_path} not found for prediction file {pred_file}.\")\n",
    "\n",
    "# Calculate and print average metrics\n",
    "average_dice = np.mean(dice_scores)\n",
    "average_iou = np.mean(iou_scores)\n",
    "average_accuracy = np.mean(pixel_accuracies)\n",
    "average_precision = np.mean(precisions)\n",
    "average_recall = np.mean(recalls)\n",
    "average_f1 = np.mean(f1_scores)\n",
    "average_specificity = np.mean(specificities)\n",
    "\n",
    "print(\"Average Metrics:\")\n",
    "print(f\"  Dice Coefficient: {average_dice:.4f}\")\n",
    "print(f\"  IoU: {average_iou:.4f}\")\n",
    "print(f\"  Pixel Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"  Precision: {average_precision:.4f}\")\n",
    "print(f\"  Recall: {average_recall:.4f}\")\n",
    "print(f\"  F1 Score: {average_f1:.4f}\")\n",
    "print(f\"  Specificity: {average_specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average metrics:\")\n",
    "print(f\"  Average Dice Coefficient: {average_dice:.4f}\")\n",
    "print(f\"  Average IoU: {average_iou:.4f}\")\n",
    "print(f\"  Average Pixel Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"  Average Precision: {average_precision:.4f}\")\n",
    "print(f\"  Average Recall: {average_recall:.4f}\")\n",
    "print(f\"  Average F1 Score: {average_f1:.4f}\")\n",
    "print(f\"  Average Specificity: {average_specificity:.4f}\")\n",
    "\n",
    "# Model's overall accuracy\n",
    "print(f\"Model's Overall Accuracy: {average_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
